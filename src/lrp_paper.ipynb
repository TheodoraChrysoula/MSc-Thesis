{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71525800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a393061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDMNIST v2.1.0 @ https://github.com/MedMNIST/MedMNIST/\n",
      "The number of classes is: 2 \n",
      " The number of channels is 1.\n",
      "The class_names are {'0': 'normal', '1': 'pneumonia'}\n",
      "{'python_class': 'PneumoniaMNIST', 'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1', 'MD5': '28209eda62fecd6e6a2d98b1501bb15f', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'pneumonia'}, 'n_channels': 1, 'n_samples': {'train': 4708, 'val': 524, 'test': 624}, 'license': 'CC BY 4.0'}\n",
      "Using downloaded and verified file: C:\\Users\\Theodora\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Theodora\\.medmnist\\pneumoniamnist.npz\n",
      "Dataset PneumoniaMNIST (pneumoniamnist)\n",
      "    Number of datapoints: 4708\n",
      "    Root location: C:\\Users\\Theodora\\.medmnist\n",
      "    Split: train\n",
      "    Task: binary-class\n",
      "    Number of channels: 1\n",
      "    Meaning of labels: {'0': 'normal', '1': 'pneumonia'}\n",
      "    Number of samples: {'train': 4708, 'val': 524, 'test': 624}\n",
      "    Description: The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
      "    License: CC BY 4.0\n",
      "==================================================\n",
      "Dataset PneumoniaMNIST (pneumoniamnist)\n",
      "    Number of datapoints: 624\n",
      "    Root location: C:\\Users\\Theodora\\.medmnist\n",
      "    Split: test\n",
      "    Task: binary-class\n",
      "    Number of channels: 1\n",
      "    Meaning of labels: {'0': 'normal', '1': 'pneumonia'}\n",
      "    Number of samples: {'train': 4708, 'val': 524, 'test': 624}\n",
      "    Description: The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
      "    License: CC BY 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\medmnist\\utils.py:25: FutureWarning: `multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  montage_arr = skimage_montage(sel_img, multichannel=(n_channels == 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[631 391 504]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import train_dataset, test_dataset, train_loader, test_loader, task, n_channels, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41321a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc248f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(nn.Module):                                                        #custom Dropout layer for saving the parameters, to be used in LRP\n",
    "    def __init__(self, p=0.5):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.seed = 0\n",
    "    \n",
    "    def forward(self, input, freeze = False):\n",
    "        if not self.training:                                                      # if model.eval(), don't apply dropout\n",
    "            return input\n",
    "\n",
    "        if not freeze:                                                             # if freeze == False, generate new random seed\n",
    "            q=np.random.randint(10000000, size = 1)[0]                             # if freeze == True , use random seed from previous run (used in LRP)\n",
    "            self.seed = q\n",
    "        \n",
    "        torch.manual_seed(self.seed)   \n",
    "        return torch.nn.functional.dropout2d(input, p=self.p)\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, droprate=0.5):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('conv1', nn.Conv2d(n_channels, 24, 5))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('maxpool1', nn.MaxPool2d(2))\n",
    "        self.model.add_module('conv2', nn.Conv2d(24, 48, kernel_size=5))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('dropout2', MyDropout())\n",
    "        self.model.add_module('maxpool2', nn.MaxPool2d(2))\n",
    "        self.model.add_module('flatten', nn.Flatten())\n",
    "        self.model.add_module('dense3', nn.Linear(48*5*5, 240))\n",
    "        self.model.add_module('relu3', nn.ReLU())\n",
    "        self.model.add_module('dropout3', MyDropout())\n",
    "        self.model.add_module('final', nn.Linear(240, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = LeNet(n_classes, n_channels)\n",
    "# model.load_state_dict(torch.load('mnist_lenet.ckpt', map_location=torch.device('cpu')))                           #load model from file\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf929b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(n_channels, n_classes)\n",
    "model.load_state_dict(torch.load('../outputs/pneumonia_mnist_lenet_2.ckpt', map_location=torch.device('cpu')))                           #load model from file\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb9c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# LRP Epsilon rule\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# More information at http://www.heatmapping.org/tutorial/\n",
    "\n",
    "def LRP_epsilon(image, class_id, model, dropout = True, verbose = False, device = 'cpu', epsilon = 1e-9):\n",
    "\n",
    "\n",
    "    # image:        Original Image,\n",
    "    # class_id:     Index of a true class,\n",
    "    # model:        Neural Network,\n",
    "    # dropout:      If set True -- run LRP-Epsilon on a sample from posterior, else run Standard LRP-Epsilon,\n",
    "    # verbose:      If set True -- print top 3 output scores,\n",
    "    # device:       Default device,\n",
    "    # epsilon:      Value of epsilon,\n",
    "   \n",
    "    if not dropout:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    X = image.view([1,1,32,32]).to(device)\n",
    "    layers = list(model.modules())[2:]\n",
    "    L = len(layers)\n",
    "\n",
    "\n",
    "    A = [X]+[None]*L\n",
    "    with torch.no_grad():\n",
    "      for l in range(L): A[l+1] = layers[l].forward(A[l]).to(device)\n",
    "    \n",
    "    scores = np.array(A[-1].data.view(-1).cpu())\n",
    "    ind = np.argsort(-scores) # sorts the output classes by max probability: array of 10 indices\n",
    "    \n",
    "    if verbose:\n",
    "        for i in ind[:3]:\n",
    "            print('New instance:')\n",
    "            print('%20s (%3d): %6.3f'%(imgclasses[i][:20],i,scores[i]))\n",
    "\n",
    "    T = A[-1].cpu().detach().numpy().tolist()[0]\n",
    "    index = T.index(max(T))\n",
    "    T = np.abs(np.array(T))*0\n",
    "    T[index]=1\n",
    "    #T = torch.FloatTensor((1.*(np.arange(2)==class_id).reshape(A[-1].shape))).to(device) # reform to one hot vector\n",
    "    R = [None] * L + [(A[-1]*T)] # R is None*12 + A[-1], only the prob of the max prob, all other elements 0 value\n",
    "\n",
    "    for l in range(0,L)[::-1]:\n",
    "        A[l] = A[l].requires_grad_(True)\n",
    "        rho = lambda p: p;                       incr = lambda z: z + epsilon\n",
    "\n",
    "        if isinstance(layers[l],torch.nn.MaxPool2d): layers[l] = torch.nn.AvgPool2d(2)\n",
    "        \n",
    "        if isinstance(layers[l],torch.nn.Conv2d) or \\\n",
    "           isinstance(layers[l],torch.nn.AvgPool2d) or \\\n",
    "           isinstance(layers[l],torch.nn.Linear) or \\\n",
    "           isinstance(layers[l],torch.nn.Flatten):\n",
    "\n",
    "            z = incr(newlayer(layers[l],rho).forward(A[l]))        # step 1\n",
    "            s = (R[l+1]/z).data                                    # step 2\n",
    "            (z*s).sum().backward(); c = A[l].grad                  # step 3\n",
    "            R[l] = (A[l]*c).data                                   # step 4\n",
    "            \n",
    "        else:\n",
    "            if not dropout:\n",
    "              R[l] = R[l+1]\n",
    "            else:\n",
    "              if isinstance(layers[l],MyDropout):\n",
    "                  incr = lambda z: z + epsilon\n",
    "                  z = incr(layers[l].forward(A[l], freeze = True))       # step 1: we freeze random state so we can remember which neurons were dropped\n",
    "                  s = (R[l+1]/z).data                                    # step 2\n",
    "                  (z*s).sum().backward(); c = A[l].grad                  # step 3\n",
    "                  R[l] = (A[l]*c).data                                   # step 4\n",
    "              else:\n",
    "                  R[l] = R[l+1]\n",
    "\n",
    "    return R[0].data\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Clone a layer and pass its parameters through the function g\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def newlayer(layer,g):\n",
    "\n",
    "    layer = copy.deepcopy(layer)\n",
    "\n",
    "    try: layer.weight = nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    try: layer.bias   = nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    return layer\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Function for MinMax Normalisation of Relevances\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def normalise_relevance(relevance_matrix):\n",
    "    a = relevance_matrix.min()\n",
    "    b = relevance_matrix.max()\n",
    "    \n",
    "    if (a == 0.) & (b == 0.):\n",
    "        return relevance_matrix\n",
    "    if (a > 0.):\n",
    "      return (relevance_matrix >0.)*relevance_matrix/b\n",
    "    if (b < 0.):\n",
    "      return - (relevance_matrix <=0.)*relevance_matrix/a\n",
    "    \n",
    "    return (relevance_matrix >0.)*relevance_matrix/b  - (relevance_matrix <=0.)*relevance_matrix/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295bfa7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[32, 32]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-36b133dbe999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_MC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mLRPs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRP_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mStandard_LRPs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRP_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[32, 32]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "N_MC = 100\n",
    "samples = np.random.choice(len(test_dataset), 4)\n",
    "LRPs = torch.zeros([len(samples), 32, 32])\n",
    "Standards_LRPs = torch.zeros([len(samples), 32, 32])\n",
    "\n",
    "counter = 0\n",
    "for q in samples:\n",
    "    for i in tqdm(range(N_MC)):\n",
    "        LRPs[counter][i] = LRP_epsilon(test_dataset[q][0].to(device), test_dataset[q][1], model, dropout=True)[0][0].data\n",
    "    Standard_LRPs[counter] = LRP_epsilon(test_dataset[q][0].to(device), test_dataset[q][1], model, dropout = False)[0][0].data\n",
    "    counter += 1\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    " T = torch.FloatTensor((1.*(np.arange(2)==class_id).reshape(A[-1].shape))).to(device) # reform to one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b57c9bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "134b9078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1.*np.arange(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2fc9f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b7ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
